# -*- coding: utf-8 -*-
"""PancreaticCancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qvDlz6fbKSMDMEUPt_xLv2PocxvGvDLC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('Debernardi et al 2020 data.csv')
df.head()

df.isna().sum()

df.describe()

df['stage']=df['stage'].fillna(0)

df.tail()

le = LabelEncoder()

df['sex'] = le.fit_transform(df['sex'])
df['patient_cohort'] = le.fit_transform(df['patient_cohort'])
df['sample_origin'] = le.fit_transform(df['sample_origin'])
#df['stage'] = le.fit_transform(df['stage'])
df['stage'] = df['stage'].replace('I',1)
df['stage'] = df['stage'].replace('II',2)
df['stage'] = df['stage'].replace('III',3)
df['stage'] = df['stage'].replace('IV',4)

df['REG1A']=df['REG1A'].fillna(df['REG1A'].mean())
df['plasma_CA19_9']=df['plasma_CA19_9'].fillna(df['plasma_CA19_9'].mean())
df.tail(399)

df.describe()

df['diagnosis'] = df['diagnosis'].replace(1,0)
df['diagnosis'] = df['diagnosis'].replace(2,0)
df['diagnosis'] = df['diagnosis'].replace(3,1)

df.drop('sample_id',axis=1,inplace=True)
df.drop('benign_sample_diagnosis',axis=1,inplace=True)

#df['stage'] = df['stage'].replace(4,0)
#df['stage'] = df['stage'].replace(0,1)
#df['stage'] = df['stage'].replace(1,2)
#df['stage'] = df['stage'].replace(2,3)
#df['stage'] = df['stage'].replace(3,4)

#df.loc[df['diagnosis'] == 0, 'stage'] = '0'

df.corr()

df.describe()

fdf = df[['diagnosis','patient_cohort','sample_origin','age','sex','creatinine','LYVE1','REG1B','TFF1','plasma_CA19_9','REG1A']]
fdf.describe()

fdf.tail()

correlation_matrix = fdf.corr()
plt.figure(figsize=(11, 11))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')

plt.show()

sns.countplot(data = fdf, x = 'diagnosis')

pd.crosstab(df.diagnosis,df.stage)

#fdf_temporary = fdf[fdf['diagnosis']==1]
sns.countplot(data=fdf , x = 'stage')

pd.crosstab(df.sex,df.diagnosis)

data = pd.crosstab(df.sex,df.diagnosis)
data.plot(kind='bar',stacked=False)
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(df['age'], kde=True, color='blue', bins=15)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

fdf.hist(column='age' , by = 'diagnosis')
plt.show()

data = pd.crosstab(df.patient_cohort,df.diagnosis)
data.plot(kind='bar',stacked=False)
plt.show()

print("Distribution of numerical columns")
fdf.hist(figsize=(10, 10), bins=20)
plt.tight_layout()
plt.show()

columns = ['creatinine', 'TFF1', 'REG1B', 'LYVE1']

# Create subplots with one row and multiple columns
fig, axes = plt.subplots(1, len(columns), figsize=(20, 5), sharey=True)

for ax, col in zip(axes, columns):
    df[col].plot(kind='hist', bins=15, ax=ax, title=col, color='skyblue')
    ax.spines[['top', 'right']].set_visible(False)
    ax.set_xlabel(col)
    ax.set_ylabel('Frequency')

# Adjust spacing
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(fdf[fdf['diagnosis'] == 0]['creatinine'], fdf[fdf['diagnosis'] == 0]['LYVE1'], label='Diagnosis 0', alpha=0.7)
plt.scatter(fdf[fdf['diagnosis'] == 1]['creatinine'], fdf[fdf['diagnosis'] == 1]['LYVE1'], label='Diagnosis 1', alpha=0.7)
plt.title('Creatinine vs LYVE1 by Diagnosis')
plt.xlabel('Creatinine')
plt.ylabel('LYVE1')
_ = plt.legend()

biomarkers = ['LYVE1', 'REG1B', 'TFF1', 'creatinine']
plt.figure(figsize=(12, 8))
for i, biomarker in enumerate(biomarkers, start=1):
    plt.subplot(2, 2, i)
    sns.boxplot(data=df, x='diagnosis', y=biomarker, palette='coolwarm')
    plt.title(f'{biomarker} Levels by Diagnosis')
    plt.xlabel('Diagnosis')
    plt.ylabel(f'{biomarker} Level')
plt.tight_layout()
plt.show()

numeric_df = fdf.select_dtypes(include=['number'])

if 'category_column' in fdf.columns:
    print("\nCount plot for categorical data:")
    sns.countplot(data=fdf, x='category_column')
    plt.show()

# Pairplot to check relationships between features
print("\nPairplot to see pairwise relationships:")
sns.pairplot(fdf)
plt.show()

# Select the biomarkers and add diagnosis for grouping
selected_data = df[['LYVE1', 'REG1B', 'TFF1', 'creatinine', 'diagnosis']]

# Create a pair plot
sns.pairplot(data=selected_data, hue='diagnosis', palette='coolwarm', diag_kind='kde')

# Display the plot
plt.suptitle('Pair Plot of Biomarkers with Diagnosis Hue', y=1.02, fontsize=16)
plt.show()

biomarkers = ['LYVE1', 'REG1B', 'TFF1', 'creatinine','plasma_CA19_9','REG1A']
plt.figure(figsize=(12, 8))
for i, biomarker in enumerate(biomarkers, start=1):
    plt.subplot(3, 2, i)
    sns.violinplot(data=df, x='diagnosis', y=biomarker, palette='muted', inner='quartile')
    plt.title(f'{biomarker} Levels by Diagnosis')
    plt.xlabel('Diagnosis')
    plt.ylabel(f'{biomarker} Level')
plt.tight_layout()
plt.show()

biomarkers = ['LYVE1', 'REG1B', 'TFF1', 'creatinine']
plt.figure(figsize=(12, 8))
for i, biomarker in enumerate(biomarkers, start=1):
    plt.subplot(2, 2, i)
    sns.regplot(data=df, x='age', y=biomarker)
    plt.title(f'{biomarker} Levels by Age')
    plt.xlabel('Age')
    plt.ylabel(f'{biomarker} Level')
plt.tight_layout()
plt.show()

fdf_temporary2 = fdf[['diagnosis','age','LYVE1','REG1B','TFF1']]
fdf_temporary2.corr()

random2 = fdf[['diagnosis','age','creatinine','LYVE1','REG1B','TFF1']]
groups = random2.groupby('diagnosis').mean()
groups.style.highlight_max(color='red')

random3 = fdf[['diagnosis','stage','age','creatinine','LYVE1','REG1B','TFF1']]
groups2 = random3.groupby('stage').mean()
groups2.drop(columns = 'diagnosis').style.highlight_max(color='red')

fdf_diag = fdf[['diagnosis','sample_origin','age','plasma_CA19_9','LYVE1','REG1B','TFF1']]

correlation_matrix = fdf_diag.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')

plt.show()

fdf_diag.head()

pip install catboost

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn import metrics
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier, StackingClassifier
import xgboost as xgb
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

x_diag = fdf_diag.drop('diagnosis', axis=1)
y_diag = fdf_diag['diagnosis']

smote = SMOTE(random_state=42)
x_diag_resampled, y_diag_resampled = smote.fit_resample(x_diag, y_diag)

print("Original class distribution:", y_diag.value_counts())
print("Resampled class distribution:", pd.Series(y_diag_resampled).value_counts())

x_diag.head()

"""**DECISION TREE**

---


"""

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)
'''
clf = DecisionTreeClassifier(
    criterion='gini',       # Use Gini Impurity
    max_depth=3,            # Limit tree depth
    min_samples_split=4,    # Minimum samples to split
    min_samples_leaf=2,     # Minimum samples in leaf
    max_features='sqrt',    # Use sqrt of total features
    random_state=42         # For reproducibility
)
'''
dt_model = tree.DecisionTreeClassifier(criterion='gini',max_features='sqrt',splitter='best',max_depth=2)
dt_model.fit(X_train, y_train)

y_pred = dt_model.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, dt_model.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, dt_model.predict(X_train))))

y_test_pred = dt_model.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

import pickle

model_df = "model_cel1_dt.pkl"
with open(model_df, "wb") as file:
    pickle.dump(dt_model, file)

"""**RANDOM FOREST**

---


"""

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, rf_model.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, rf_model.predict(X_train))))
y_test_pred = rf_model.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

'''
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
'''

rf_model = RandomForestClassifier(max_depth=20,max_features='sqrt',min_samples_leaf=1,min_samples_split=5,n_estimators=300)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, rf_model.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, rf_model.predict(X_train))))
y_test_pred = rf_model.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

"""**LIGHT GBM**

---


"""

'''
gbm_clf = LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=-1,  # No limit on depth
    num_leaves=31,  # Number of leaves in each tree
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
'''

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

lgb_model = lgb.LGBMClassifier(n_estimators=100,
                               learning_rate = 0.1,
                               max_depth=3,
                               num_leaves=31,
                               subsamples=0.8,
                               colsample_bytree=0.8,
                               random_state=42,
                               verbosity=-1)
lgb_model.fit(X_train, y_train)

y_pred = lgb_model.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, lgb_model.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, lgb_model.predict(X_train))))
y_test_pred = lgb_model.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

"""**ADA BOOST**

---


"""

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

adaboost_clf = AdaBoostClassifier(n_estimators=50,  # Number of weak learners
    learning_rate=1.0,  # Step size
    random_state=42
)
adaboost_clf.fit(X_train, y_train)

y_pred = adaboost_clf.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, adaboost_clf.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, adaboost_clf.predict(X_train))))
y_test_pred = adaboost_clf.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

"""**GRADIENT BOOSTING**

---


"""

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

gradient_clf = GradientBoostingClassifier(
    n_estimators=100,  # Number of trees
    learning_rate=0.1,  # Step size
    max_depth=3,  # Depth of each tree
    random_state=42
)
gradient_clf.fit(X_train, y_train)

y_pred = gradient_clf.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, gradient_clf.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, gradient_clf.predict(X_train))))
y_test_pred = gradient_clf.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

"""**CAT BOOST**

---


"""

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

catboost_clf = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=6,
    random_seed=42,
    verbose=0
)
catboost_clf.fit(X_train, y_train)

y_pred = catboost_clf.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, catboost_clf.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, catboost_clf.predict(X_train))))
y_test_pred = catboost_clf.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

"""**XG BOOST**

---


"""

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

xgb_clf = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
)
xgb_clf.fit(X_train, y_train)

y_pred = xgb_clf.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, xgb_clf.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, xgb_clf.predict(X_train))))
y_test_pred = xgb_clf.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))
'''
model_xgb = "model_cell_xgb.pkl"
with open(model_xgb, "wb") as file:
    pickle.dump(xgb_clf, file)
'''

"""**KNN**

---

"""

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

knn_model = KNeighborsClassifier(n_neighbors=3)  # Use optimal k based on your data
knn_model.fit(X_train, y_train)

y_pred = knn_model.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", conf_matrix)

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, knn_model.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, knn_model.predict(X_train))))
y_test_pred = knn_model.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

"""**STACKING**

---


"""

from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

log_reg = LogisticRegression(random_state=42, max_iter=1000)
rf = RandomForestClassifier(random_state=42)
svc = SVC(probability=True, random_state=42)
knn = KNeighborsClassifier()
nb = GaussianNB()
gb = GradientBoostingClassifier(random_state=42)

base_learners = [
    ('rf', RandomForestClassifier(random_state=42)),
    ('gb', GradientBoostingClassifier(random_state=42)),
    ('svc', SVC(probability=True, random_state=42)),
    ('knn', KNeighborsClassifier()),
    ('nb', GaussianNB())
]
meta_model = LogisticRegression(max_iter=1000, random_state=42)

stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_model)
stacking_clf.fit(X_train, y_train)
y_pred = stacking_clf.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, stacking_clf.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, stacking_clf.predict(X_train))))
y_test_pred = stacking_clf.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

"""**VOTING**

---


"""

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

voting_clf = VotingClassifier(estimators=[
    ('log_reg', log_reg),
    ('rf', rf),
    ('svc', svc),
    ('knn', knn),
    ('nb', nb),
    ('gb', gb)
], voting='soft')

voting_clf.fit(X_train, y_train)
y_pred = voting_clf.predict(X_test)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, voting_clf.predict(X_train))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, voting_clf.predict(X_train))))
y_test_pred = voting_clf.predict(X_test)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

"""**BOOSTING**

---


"""

X_train, X_test, y_train, y_test = train_test_split(x_diag_resampled, y_diag_resampled, test_size=0.3,shuffle=True,stratify=y_diag_resampled)
#X_train, X_test, y_train, y_test = train_test_split(x_diag, y_diag, test_size=0.3,shuffle=True,stratify=y_diag)

dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters for XGBoost (multiclass)
params = {
    'objective': 'multi:softmax',  # multiclass classification
    'num_class': len(y_diag_resampled.unique()),  # number of classes in the target
    'eval_metric': 'mlogloss',
    'max_depth': 6,
    'learning_rate': 0.1,
    'n_estimators': 100
}

bst = xgb.train(params, dtrain, num_boost_round=100)

y_pred = bst.predict(dtest)

print("\nClassification Report:\n", classification_report(y_test, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train, bst.predict(dtrain))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train, bst.predict(dtrain))))
y_test_pred = bst.predict(dtest)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test, y_test_pred)))

"""**STAGE CLASSIFICATION**

---



---


"""

fdf.tail()

fdf.head()

counts = fdf['stage'].value_counts()
print(counts)

#fdf.loc[:end_index, 'column_name'] = new_value
#print(fdf.loc[586,'stage'])

df_stage = fdf[fdf['diagnosis']==1]
df_stage.tail(100)
#df_stage.corr()

df_stage.describe()

fdf_stage = df_stage.drop('diagnosis', axis=1)

fdf_stage.corr()

x_stage = fdf_stage.drop(columns=['stage','patient_cohort','sample_origin','plasma_CA19_9','age','sex','REG1B'],axis=1)
y_stage = fdf_stage['stage']
x_stage.head()

smote = SMOTE(random_state=42)
x_resampled_stage, y_resampled_stage = smote.fit_resample(x_stage, y_stage)

print("Original class distribution:", y_stage.value_counts())
print("Resampled class distribution:", pd.Series(y_resampled_stage).value_counts())

"""**DECISION TREE**

---


"""

X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_resampled_stage, y_resampled_stage, test_size=0.3,shuffle=True,stratify=y_resampled_stage)
#X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_stage, y_stage, test_size=0.3,shuffle=True,stratify=y_stage)
'''
riterion='gini',       # Use Gini Impurity
    max_depth=3,            # Limit tree depth
    min_samples_split=4,    # Minimum samples to split
    min_samples_leaf=2,     # Minimum samples in leaf
    max_features='sqrt',    # Use sqrt of total features
    random_state=42
'''
dt_model = tree.DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=5,min_samples_split=4,min_samples_leaf=2)
dt_model.fit(X_train_stage, y_train_stage)

y_pred = dt_model.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, dt_model.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, dt_model.predict(X_train_stage))))

y_test_pred = dt_model.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

"""**RANDOM FOREST**

---


"""

X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_resampled_stage, y_resampled_stage, test_size=0.3,shuffle=True,stratify=y_resampled_stage)
#X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_stage, y_stage, test_size=0.3,shuffle=True,stratify=y_stage)

rf_model2 = RandomForestClassifier(n_estimators=200,class_weight='balanced_subsample',criterion='gini',max_features='sqrt',bootstrap=True,max_depth=10,min_samples_split=5)
rf_model2.fit(X_train_stage, y_train_stage)

y_pred = rf_model2.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, rf_model2.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, rf_model2.predict(X_train_stage))))
y_test_pred = rf_model2.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

'''
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50,100, 200, 300,400],
    'max_depth': [5,10,12,15,20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4,5],
    'max_features': ['sqrt', 'log2'],
    'class_weight': ['balanced', 'balanced_subsample']
}

grid_search = GridSearchCV(estimator=rf_model2, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train_stage, y_train_stage)

print("Best Parameters:", grid_search.best_params_)
'''

X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_resampled_stage, y_resampled_stage, test_size=0.3,shuffle=True,stratify=y_resampled_stage)
#X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_stage, y_stage, test_size=0.3,shuffle=True,stratify=y_stage)

rf_model2 = RandomForestClassifier(n_estimators=50,class_weight='balanced',criterion='gini',max_features='log2',max_depth=10,min_samples_split=2,min_samples_leaf=1)
rf_model2.fit(X_train_stage, y_train_stage)

y_pred = rf_model2.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, rf_model2.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, rf_model2.predict(X_train_stage))))
y_test_pred = rf_model2.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

"""**LIGHT GBM**

---


"""

X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_resampled_stage, y_resampled_stage, test_size=0.3,shuffle=True,stratify=y_resampled_stage)
#X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_stage, y_stage, test_size=0.3,shuffle=True,stratify=y_stage)

lgb_model = lgb.LGBMClassifier(n_estimators=100,
                               learning_rate = 0.1,
                               max_depth=3,
                               num_leaves=31,
                               subsamples=0.8,
                               colsample_bytree=0.8,
                               random_state=42,
                               verbosity=-1)
lgb_model.fit(X_train_stage, y_train_stage)

y_pred = lgb_model.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, lgb_model.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, lgb_model.predict(X_train_stage))))
y_test_pred = lgb_model.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

"""**ADA BOOST**

---


"""

X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_resampled_stage, y_resampled_stage, test_size=0.3,shuffle=True,stratify=y_resampled_stage)
#X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(x_stage, y_stage, test_size=0.3,shuffle=True,stratify=y_stage)

adaboost_clf = AdaBoostClassifier(n_estimators=50,  # Number of weak learners
    learning_rate=1.0,  # Step size
    random_state=42
)
adaboost_clf.fit(X_train_stage, y_train_stage)

y_pred = adaboost_clf.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, adaboost_clf.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, adaboost_clf.predict(X_train_stage))))
y_test_pred = adaboost_clf.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

"""**GRADIENT BOOSTING**

---


"""

gradient_clf = GradientBoostingClassifier(
    n_estimators=100,  # Number of trees
    learning_rate=0.1,  # Step size
    max_depth=3,  # Depth of each tree
    random_state=42
)
gradient_clf.fit(X_train_stage, y_train_stage)

y_pred = gradient_clf.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, gradient_clf.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, gradient_clf.predict(X_train_stage))))
y_test_pred = gradient_clf.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

"""**CAT BOOST**

---


"""

catboost_clf = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=6,
    random_seed=42,
    verbose=0
)
catboost_clf.fit(X_train_stage, y_train_stage)

y_pred = catboost_clf.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, catboost_clf.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, catboost_clf.predict(X_train_stage))))
y_test_pred = catboost_clf.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

"""**XG BOOST**

---


"""

xgb_clf = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
y_train_stage_encoded = le.fit_transform(y_train_stage)
y_test_stage_encoded = le.fit_transform(y_test_stage)
xgb_clf.fit(X_train_stage, y_train_stage_encoded)

y_pred = xgb_clf.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage_encoded, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage_encoded, xgb_clf.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage_encoded, xgb_clf.predict(X_train_stage))))
y_test_pred = xgb_clf.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage_encoded, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage_encoded, y_test_pred)))

"""**KNN**

---


"""

knn_model = KNeighborsClassifier(n_neighbors=3)  # Use optimal k based on your data
knn_model.fit(X_train_stage, y_train_stage)

y_pred = knn_model.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

conf_matrix = confusion_matrix(y_test_stage, y_pred)
print("\nConfusion Matrix:\n", conf_matrix)

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, knn_model.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, knn_model.predict(X_train_stage))))
y_test_pred = knn_model.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

"""**STACKING**

---


"""

base_learners = [
    ('rf', RandomForestClassifier(random_state=42)),
    ('gb', GradientBoostingClassifier(random_state=42)),
    ('svc', SVC(probability=True, random_state=42)),
    ('knn', KNeighborsClassifier()),
    ('nb', GaussianNB())
]
meta_model = LogisticRegression(max_iter=1000, random_state=42)

# Create the Stacking Classifier
stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_model)

# Train the model
stacking_clf.fit(X_train_stage, y_train_stage)

# Make predictions
y_pred = stacking_clf.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, stacking_clf.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, stacking_clf.predict(X_train_stage))))
y_test_pred = stacking_clf.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

"""**VOTING**

---


"""

voting_clf = VotingClassifier(estimators=[
    ('log_reg', log_reg),
    ('rf', rf),
    ('svc', svc),
    ('knn', knn),
    ('nb', nb),
    ('gb', gb)
], voting='soft')

voting_clf.fit(X_train_stage, y_train_stage)

# Make predictions
y_pred = voting_clf.predict(X_test_stage)

print("\nClassification Report:\n", classification_report(y_test_stage, y_pred))

print('Training Accuracy = {}'.format(metrics.accuracy_score(y_train_stage, voting_clf.predict(X_train_stage))))
print('Training Confusion = \n{}'.format(metrics.confusion_matrix(y_train_stage, voting_clf.predict(X_train_stage))))
y_test_pred = voting_clf.predict(X_test_stage)

print('Testing Accuracy = {}'.format(metrics.accuracy_score(y_test_stage, y_test_pred)))
print('Testing Confusion = \n{}'.format(metrics.confusion_matrix(y_test_stage, y_test_pred)))

"""**BOOSTING**

---


"""